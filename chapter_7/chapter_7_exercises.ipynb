{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Exercises\n",
    "\n",
    "This notebook holds the code for the chapter 7 exercises that require coding\n",
    "\n",
    "Question 8:\n",
    "- Load MNIST, split into: train, test, validation (40,000, 10,000, 10,000)\n",
    "- Train various classifiers (RF, Extra-Trees, SVM)\n",
    "- Combine the above classifiers into an ensemble that outperforms on validation set, use hard or soft voting classifier. \n",
    "- Try the ensemble on the test set, how much better is it than individual classifiers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 40000\n",
    "VALID_SIZE = 10000\n",
    "TEST_SIZE = 10000\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "ROUND_TO = 4\n",
    "MAX_ITER = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(type_of_data):\n",
    "    mnist_data = fetch_mldata(type_of_data)\n",
    "\n",
    "    X_train, X_test_valid, y_train, y_test_valid = train_test_split(mnist_data.data, \n",
    "                                                    mnist_data.target, \n",
    "                                                    test_size=TEST_SIZE + VALID_SIZE,\n",
    "                                                    train_size=TRAIN_SIZE,\n",
    "                                                    random_state=RANDOM_STATE)\n",
    " \n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_test_valid, \n",
    "                                                    y_test_valid, \n",
    "                                                    test_size=TEST_SIZE, \n",
    "                                                    random_state=RANDOM_STATE)\n",
    "    \n",
    "    all_data = {\n",
    "        'X_train': X_train, \n",
    "        'X_valid': X_valid, \n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train, \n",
    "        'y_valid': y_valid, \n",
    "        'y_test': y_test\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, \n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion Matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_classifier(clf, data, round_to=4, classifier_name='', plot_cnf_mat=False):\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_valid = data['X_valid']\n",
    "    y_valid = data['y_valid']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    if classifier_name == 'svm':\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_valid = scaler.transform(X_valid)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    valid_predictions = clf.predict(X_valid)\n",
    "    valid_cnf_mat = confusion_matrix(y_valid, valid_predictions) \n",
    "    valid_accuracy = sum(np.diag(valid_cnf_mat)) / sum(valid_cnf_mat.flatten())\n",
    "    valid_accuracy_r = round(valid_accuracy, round_to)\n",
    "    \n",
    "    test_predictions = clf.predict(X_test)\n",
    "    test_cnf_mat = confusion_matrix(y_test, test_predictions)\n",
    "    test_accuracy = sum(np.diag(test_cnf_mat)) / sum(test_cnf_mat.flatten())\n",
    "    test_accuracy_r = round(test_accuracy, round_to)\n",
    "    \n",
    "    title = 'The {0} accuracy is {1:.3f}'.format(classifier_name, test_accuracy)\n",
    "    \n",
    "    if plot_cnf_mat:\n",
    "        plot_confusion_matrix(svm_cnf_mat, classes=np.unique(y_train), title=svm_title, normalize=True)\n",
    "    \n",
    "    return clf, valid_predictions, valid_accuracy_r, test_accuracy_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinsey40/software/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/kinsey40/software/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 35s, sys: 744 ms, total: 9min 36s\n",
      "Wall time: 9min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = get_data('MNIST original')\n",
    "classifiers = {\n",
    "    'rf': RandomForestClassifier(), \n",
    "    'et': ExtraTreesClassifier(), \n",
    "    'svm': SVC(max_iter=MAX_ITER)\n",
    "}\n",
    "\n",
    "validation_accuracies = dict(zip(list(classifiers.keys()), [None] * len(classifiers)))\n",
    "test_accuracies = dict(zip(list(classifiers.keys()), [None] * len(classifiers)))\n",
    "valid_predictions = dict(zip(list(classifiers.keys()), [None] * len(classifiers)))\n",
    "vtg_valid_accuracy_diffs = dict(zip(list(classifiers.keys()), [None] * len(classifiers)))\n",
    "vtg_test_accuracy_diffs = dict(zip(list(classifiers.keys()), [None] * len(classifiers)))\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    clf, valid_prediction, validation_accuracy, test_accuracy = fit_classifier(classifier, \n",
    "                                                                               data, \n",
    "                                                                               round_to=ROUND_TO, \n",
    "                                                                               classifier_name=key)\n",
    "    validation_accuracies[key] = validation_accuracy\n",
    "    test_accuracies[key] = test_accuracy\n",
    "    valid_predictions[key] = valid_prediction\n",
    "    classifiers[key] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinsey40/software/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vtg_valid_accuray_diffs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2c6c7c59087d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvtg_test_accuracy_diffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvtg_test_accuracy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtest_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtg_valid_accuray_diffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtg_test_accuracy_diffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Voting test acc: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtg_test_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vtg_valid_accuray_diffs' is not defined"
     ]
    }
   ],
   "source": [
    "vtg_clf = VotingClassifier(list(classifiers.items()), voting='hard')\n",
    "vtg_clf, vtg_valid_prediction, vtg_validation_accuracy, vtg_test_accuracy = fit_classifier(vtg_clf, \n",
    "                                                                                           data, \n",
    "                                                                                           round_to=ROUND_TO, \n",
    "                                                                                           classifier_name='vtg')\n",
    "for key in classifiers.keys():    \n",
    "    vtg_valid_accuracy_diffs[key] = vtg_validation_accuracy - validation_accuracies[key]\n",
    "    vtg_test_accuracy_diffs[key] = vtg_test_accuracy - test_accuracies[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf': -0.0014999999999999458, 'et': -0.0057999999999999163, 'svm': -0.020299999999999985}\n",
      "{'rf': -0.0041999999999999815, 'et': -0.0097999999999999199, 'svm': -0.021299999999999986}\n",
      "Voting test acc: 0.937\n"
     ]
    }
   ],
   "source": [
    "print(vtg_valid_accuracy_diffs)\n",
    "print(vtg_test_accuracy_diffs)\n",
    "print('Voting test acc: {}'.format(vtg_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "- Run above clfs on validation set, create a new training set, with the resulting predictions: each training instance is a vector with the predictions from each classifier\n",
    "- Perform evaluation of the test set with the new ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf': array([ 8.,  0.,  8., ...,  8.,  4.,  4.]), 'et': array([ 8.,  0.,  8., ...,  8.,  4.,  4.]), 'svm': array([ 8.,  0.,  8., ...,  8.,  4.,  4.])}\n",
      "(10000, 3)\n",
      "(10000, 3)\n",
      "The blender accuracy is: 0.9583\n",
      "Therefore the difference in the performanace is: 0.021299999999999986\n"
     ]
    }
   ],
   "source": [
    "testing_preds = []\n",
    "\n",
    "for clf in classifiers.values():\n",
    "    testing_preds_clf = clf.predict(data['X_test'])\n",
    "    testing_preds.append(testing_preds_clf)\n",
    "    \n",
    "new_training_data = np.hstack(tuple([values for values in list(valid_predictions.values())])).reshape(TEST_SIZE, -1)\n",
    "new_testing_data = np.hstack(tuple(testing_preds)).reshape(TEST_SIZE, -1)\n",
    "\n",
    "blender = RandomForestClassifier()\n",
    "blender.fit(new_training_data, data['y_valid'])\n",
    "\n",
    "blender_predictions = blender.predict(new_testing_data)\n",
    "blender_cnf_mat = confusion_matrix(data['y_test'], blender_predictions)\n",
    "blender_accuracy = sum(np.diag(blender_cnf_mat)) / sum(blender_cnf_mat.flatten())\n",
    "blender_accuracy_r = round(test_accuracy, ROUND_TO)\n",
    "\n",
    "print('The blender accuracy is: {0:.3f}'.format(blender_accuracy_r))\n",
    "print('Therefore the difference in the performanace is: {0:.3f}'.format(blender_accuracy_r - vtg_test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Therefore the difference in the performanace is: 0.021\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
